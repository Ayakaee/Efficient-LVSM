model:
  class_name: model.LVSM_scene_decoder_only.Images2LatentScene
  repa_encoder_type: PE-Core-L14 # dinov2-vit-b
  projector_dim: 2048
  freeze_image_encoder: true
  align_resolution: false
  repa_config: 1
  repa_projector_type: linear3
  extra_enc: none
  enc_layer: 2
  concat_rgb: true
  distill_half_dim: false
  label2x: true
  repa_concat: false
  use_log_scale: none

  image_tokenizer:
    type: linear # dino or linear or none
    image_size: 256
    patch_size: 8
    in_channels: 9  # 3 RGB + 3 direction + 3 Reference
    use_patch_interpolation: true
    source: github
    model_path: /root/.cache/torch/hub/checkpoints/dinov2_vitb14_pretrain.pth
    model_source_dir: 

  target_pose_tokenizer:
    image_size: 256
    patch_size: 8
    in_channels: 6  # 3 direction + 3 Reference
  transformer:
    d: 768
    d_head: 64
    n_layer: 24
    special_init: true
    depth_init: true
    use_qk_norm: true
    attention_arch: flex
    input_mode : embed
    input_scope: local
    mode: self # 'self' or 'cross' or 'both'

training:
  amp_dtype: bf16
  api_key_path: ./configs/api_keys.yaml
  batch_size_per_gpu: 2
  beta1: 0.9
  beta2: 0.95
  allowed_gradnorm_factor: 10 # 5
  center_crop: true
  scene_scale_factor: 1.35
  checkpoint_dir: ./experiments/checkpoints/7.8-test
  checkpoint_every: 2000
  checkpoint_every_epoch: 1
  checkpoint_every_time: 1
  dataset_name: data.dataset_scene.Dataset
  dataset_path: data/train/full_list.txt
  train_epochs: 20
  dataset_len: 66033
  grad_accum_steps: 1
  grad_checkpoint_every: 1
  grad_clip_norm: 10 # 1.0

  enable_repa: false
  repa_stop_epoch: -1
  l2_loss_weight: 1.0
  lpips_loss_weight: 0.0
  perceptual_loss_weight: 0.5
  proj_loss_weight: 0.5
  encode_depth: 8

  lr: 0.0004
  train_steps: 20000
  train_time: 24
  use_compile: true

  use_view_masking: false
  view_mask_strategy: unified
  view_min: 2
  view_max: 2
  num_input_views: 2
  num_target_views: 6
  num_threads: 32
  num_views: 8
  num_workers: 16
  prefetch_factor: 32
  print_every: 20


  square_crop: true
  target_has_input: true
  use_amp: true
  use_rel_pose: false
  use_tf32: true
  view_selector:
    max_frame_dist: 192
    min_frame_dist: 25
  vis_every: 1000
  wandb_exp_name: LVSM_scene_decoder_only
  wandb_log_every: 50
  
  wandb_project: LVSM
  warmup: 3000
  weight_decay: 0.05
  backup_code: true


# inference / evaluation
inference:
  if_inference: False
  compute_metrics: False
  view_idx_file_path: ./data/evaluation_index_re10k.json
  render_video: False
  render_video_config:
    traj_type: interpolate
    num_frames: 120
    loop_video: True 
    order_poses: False

demo:
  rotate_x: 0
  rotate_y: 0
  rotate_z: 0
  translate_x: 0
  translate_y: 0
  translate_z: 0